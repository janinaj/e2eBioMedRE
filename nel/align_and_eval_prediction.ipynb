{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from bioc import pubtator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed dev data\n",
    "datapath = \"../data/NER/PURE/dev.json\"\n",
    "dev = []\n",
    "for line in open(datapath, 'r'):\n",
    "    dev.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "# Filter out only 100 Dev samples\n",
    "# which are same with BioRED Test set\n",
    "dev = [data for data in dev if \"Task2\" in data['doc_key']]\n",
    "doc_keys = [data[\"doc_key\"] for data in dev]\n",
    "print(len(dev), len(doc_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gold_unique_cuis = []\n",
    "\n",
    "for data in dev:\n",
    "    \n",
    "    # To store the cased version of tokens\n",
    "    data[\"cased_tokens\"] = []\n",
    "    \n",
    "    # Store entity mention using spans for future use\n",
    "    if data[\"ner\"]:\n",
    "        for annotation in data[\"ner\"]:\n",
    "            \n",
    "            if annotation['entity_type'] == \"Disease\":\n",
    "                annotation['entity_type'] = \"DiseaseOrPhenotypicFeature\"\n",
    "            \n",
    "            annotation[\"mentions\"] = []\n",
    "            for span in annotation[\"mention_spans\"]:\n",
    "                    \n",
    "                entity_tokens = data[\"tokens\"][int(span[0]):min(int(span[1]+1),len(data[\"tokens\"]))]\n",
    "                \n",
    "                entity_mention = ''\n",
    "                for idx, token in enumerate(entity_tokens):\n",
    "                    if token.startswith(\"##\"):\n",
    "                        entity_mention += token.lstrip(\"##\")\n",
    "                    else:\n",
    "                        if idx == 0:\n",
    "                            entity_mention += token\n",
    "                        else:\n",
    "                            entity_mention += \" \" + token\n",
    "                            \n",
    "                annotation[\"mentions\"].append(entity_mention)\n",
    "                    \n",
    "    # We need to make char2token mapper\n",
    "    # as PubTator3 spans are char-based, whereas the PURE models takes token-based spans\n",
    "    all_sentences = \" \".join(data['sentence_texts'])\n",
    "\n",
    "    char2token = {}    \n",
    "    chunk = \"\"\n",
    "    target_tokens = data['tokens'][1:-1]  # Exclude [CLS] and [SEP]\n",
    "    \n",
    "    uncased_chunk = \"\"\n",
    "    uncased2cased = {}\n",
    "\n",
    "    token_idx = 1\n",
    "    # assume that target_tokens are sorted by its index\n",
    "    unk_token_indices = []\n",
    "    for curr_idx, char in enumerate(all_sentences):\n",
    "        \n",
    "        if char == \" \":\n",
    "            continue\n",
    "            \n",
    "        uncased_chunk += char.lower()\n",
    "        chunk += char\n",
    "        \n",
    "        target_token = target_tokens[0].lstrip(\"##\")\n",
    "        if target_token == \"[UNK]\":\n",
    "            unk_start_idx = curr_idx\n",
    "            unk_token_indices.append(token_idx)\n",
    "            token_idx += 1\n",
    "            target_tokens = target_tokens[1:]\n",
    "            continue\n",
    "        else:         \n",
    "            if target_token in uncased_chunk:\n",
    "                if chunk != uncased_chunk:\n",
    "                    if target_tokens[0].startswith(\"##\"):\n",
    "                        uncased2cased[token_idx] = \"##\"+chunk[-len(target_token):]\n",
    "                    else:\n",
    "                        uncased2cased[token_idx] = chunk[-len(target_token):]\n",
    "                \n",
    "                if unk_token_indices:\n",
    "                    unk_chunk = uncased_chunk[:-len(target_token)]\n",
    "                    unk_chunks = unk_chunk.strip().split(\" \")\n",
    "                    if len(unk_chunks) != len(unk_token_indices):  # NOTE: temporary snippet\n",
    "                        unk_chunks = [unk_chunks[0][0], unk_chunks[0][1]]\n",
    "                        \n",
    "                    assert len(unk_chunks) == len(unk_token_indices)\n",
    "                    \n",
    "                    for unk_chunk, unk_token_idx in zip(unk_chunks, unk_token_indices):\n",
    "                        char2token[(unk_start_idx, unk_start_idx+len(unk_chunk))] = unk_token_idx\n",
    "                        unk_start_idx = unk_start_idx+len(unk_chunk)\n",
    "                        \n",
    "                    unk_token_indices = []\n",
    "                    \n",
    "                char2token[(curr_idx-len(target_token)+1, curr_idx+1)] = token_idx\n",
    "                chunk = \"\"\n",
    "                uncased_chunk = \"\"\n",
    "                token_idx += 1\n",
    "                target_tokens = target_tokens[1:]\n",
    "\n",
    "    if target_token == \"[UNK]\":\n",
    "        char2token[(unk_start_idx, len(all_sentences))] = unk_token_indices[0]\n",
    "        unk_token_indices = []\n",
    "        \n",
    "    assert token_idx == len(data['tokens'])-1\n",
    "    \n",
    "    data['char2token'] = char2token\n",
    "    data['token2char'] = {v:k for k, v in char2token.items()}\n",
    "    \n",
    "    for idx in range(len(data['tokens'])):\n",
    "        if idx in uncased2cased:\n",
    "            data[\"cased_tokens\"].append(uncased2cased[idx]) \n",
    "        else:\n",
    "            data[\"cased_tokens\"].append(data[\"tokens\"][idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>offset_start</th>\n",
       "      <th>offset_finish</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BC8_BioRED_Task2_Doc594</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>GeneOrGeneProduct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BC8_BioRED_Task2_Doc594</td>\n",
       "      <td>56</td>\n",
       "      <td>72</td>\n",
       "      <td>DiseaseOrPhenotypicFeature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC8_BioRED_Task2_Doc594</td>\n",
       "      <td>97</td>\n",
       "      <td>108</td>\n",
       "      <td>DiseaseOrPhenotypicFeature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               abstract_id  offset_start  offset_finish  \\\n",
       "0  BC8_BioRED_Task2_Doc594             8             13   \n",
       "1  BC8_BioRED_Task2_Doc594            56             72   \n",
       "2  BC8_BioRED_Task2_Doc594            97            108   \n",
       "\n",
       "                         type  \n",
       "0           GeneOrGeneProduct  \n",
       "1  DiseaseOrPhenotypicFeature  \n",
       "2  DiseaseOrPhenotypicFeature  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Predicted NER from PURE\n",
    "pred_ner_path = '../ner/models/final_model/postprocessed_predictions.csv'\n",
    "pred_df = pd.read_csv(pred_ner_path, delimiter=\"\\t\").iloc[:, 1:]\n",
    "\n",
    "# # If you are to use Gold NER, change the path\n",
    "# pred_df = pd.read_csv(\"./data/ner_output/dev_gold_ner.csv\", delimiter=\"\\t\").iloc[:, 1:]\n",
    "print(len(pred_df))\n",
    "pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "preds = defaultdict(list)\n",
    "for item in pred_df.to_dict('records'):\n",
    "    if item['abstract_id'] not in doc_keys:\n",
    "        continue\n",
    "    preds[item['abstract_id']].append({\n",
    "        'abstract_id': item['abstract_id'],\n",
    "        'start': item['offset_start'],\n",
    "        'end': item['offset_finish'],\n",
    "        'type': item['type'],   \n",
    "    })\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PubTator3 Prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align name of labels btwn the BioRED and PubT3\n",
    "biored2pubt = {\n",
    "    'GeneOrGeneProduct': 'Gene',\n",
    "    'DiseaseOrPhenotypicFeature': 'Disease',\n",
    "    'OrganismTaxon': 'Species',\n",
    "    'ChemicalEntity': 'Chemical',\n",
    "    'SequenceVariant': 'SNP',\n",
    "    'CellLine': 'CellLine'\n",
    "}\n",
    "pubt2biored = {v:k for k, v in biored2pubt.items()}\n",
    "pubt2biored['DNAMutation'] = 'SequenceVariant'\n",
    "pubt2biored['ProteinMutation'] = 'SequenceVariant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp = \".outputs/pubtator/biored_task1_val_100_pubtator3_aligned.pubtator\"\n",
    "\n",
    "pubtator_files = defaultdict(list)\n",
    "\n",
    "# To make lookup dict for entity mentions based on PubT result\n",
    "pubt_cui_lookup = {}\n",
    "\n",
    "with open(fp) as f:\n",
    "    docs = pubtator.load(f)\n",
    "    for doc in docs:\n",
    "        \n",
    "        if doc.pmid not in doc_keys:\n",
    "            continue            \n",
    "            \n",
    "        # Add mentions for NER-pred file\n",
    "        ner_preds = preds[doc.pmid]\n",
    "        for pred in ner_preds:\n",
    "            pred['text'] = doc.text[pred['start']:pred['end']]\n",
    "            \n",
    "        for e in doc.annotations:\n",
    "            if e.type == 'CellLine':\n",
    "                e.id = e.id.replace(\":\", \"_\")\n",
    "            elif e.type in [\"Disease\", \"Chemical\"] and e.id.startswith('MESH'):\n",
    "                e.id = e.id.split(\":\")[-1]\n",
    "            # Parsing rules for SeqVar IDs\n",
    "            elif e.type == \"SNP\" or e.type.endswith(\"Mutation\"):\n",
    "                # Define a regular expression pattern to match the RS# and the following number\n",
    "                pattern = re.compile(r'RS#:(\\d+)')\n",
    "                matches = pattern.findall(e.id)            \n",
    "                if matches:\n",
    "                    rs_num = matches[0]\n",
    "                    e.id = f'rs{rs_num}'\n",
    "                else:\n",
    "                    e.id = e.id.split(';')[0].split(':')[1]\n",
    "\n",
    "            pubtator_files[doc.pmid].append({\n",
    "                'start':int(e.start),\n",
    "                'end':int(e.end),\n",
    "                'mention':e.text,\n",
    "                'type':e.type,\n",
    "                'cui':e.id,\n",
    "            })\n",
    "            \n",
    "            mention = e.text.lower()\n",
    "            if not e.type == \"SNP\" and not e.type.endswith(\"Mutation\"):\n",
    "                cuis = re.split(r'[,|]', e.id)\n",
    "            else:\n",
    "                cuis = [e.id]\n",
    "            for cui in cuis:\n",
    "                if e.type == 'Chromosome':\n",
    "                    continue\n",
    "                biored_type = pubt2biored[e.type]\n",
    "                lookup_key = (mention, biored_type)\n",
    "                # Lowercase key\n",
    "                if lookup_key not in pubt_cui_lookup:\n",
    "                    pubt_cui_lookup[lookup_key] = {}\n",
    "                    pubt_cui_lookup[lookup_key][cui] = 1\n",
    "                else:\n",
    "                    if cui not in pubt_cui_lookup[lookup_key]:\n",
    "                        pubt_cui_lookup[lookup_key][cui] = 1\n",
    "                    else:\n",
    "                        pubt_cui_lookup[lookup_key][cui] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build look-up dictionary from PubTator3 outputs\n",
    "pubt_cui_lookup_listed = {}\n",
    "for k, values in pubt_cui_lookup.items():\n",
    "    values = sorted([(inner_k, inner_v) for inner_k, inner_v in values.items()], key=lambda x: -x[1])\n",
    "    pubt_cui_lookup_listed[k] = values[0][0]  # Only take the most frequent one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NEL predictions from ResCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "# Map normalized queries into original mentions\n",
    "# If you have different NER prediction, you need to map mentions based on the predicted NER\n",
    "disease_map_path = './resources/data/biosyn-processed-bc8biored-disease/mention_map_disease_dev.json'\n",
    "# disease_map_path = './resources/data/biosyn-processed-bc8biored-disease/mention_map_disease_dev_gold.json'\n",
    "with open(disease_map_path) as f:\n",
    "    mention_map_disease = json.load(f)\n",
    "print(len(mention_map_disease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "chemical_map_path = './resources/data/biosyn-processed-bc8biored-disease/mention_map_chemical_dev.json'\n",
    "# chemical_map_path = './resources/data/biosyn-processed-bc8biored-disease/mention_map_chemical_dev_gold.json'\n",
    "with open(chemical_map_path) as f:\n",
    "    mention_map_chemical = json.load(f)\n",
    "print(len(mention_map_chemical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 384 {'mention': 'multiple pterygium syndrome', 'gold_id': 'C537377', 'pred_id': 'C537377|265000', 'pred_name': 'multiple pterygium syndrome'}\n"
     ]
    }
   ],
   "source": [
    "# load NEL output from ResCNN for Disease concept\n",
    "disease_output_path = './outputs/rescnn/results/bc8biored-disease-aio/lightweight_cnn_text_with_attention_pooling_biolinkbert/lr_0.001-depth_4-fs_256-drop_0.25/predictions_dev_disease.json'\n",
    "with open(disease_output_path, 'r') as f:\n",
    "    disease_rescnn = json.load(f)\n",
    "disease_rescnn_dict = {d['mention']: d['pred_id'] for d in disease_rescnn}\n",
    "print(len(disease_rescnn), len(disease_rescnn_dict), disease_rescnn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 223 {'mention': '1 25 oh 2d3', 'gold_id': 'D002117', 'pred_id': 'D002117'}\n"
     ]
    }
   ],
   "source": [
    "# load NEL output from ResCNN for Chemical concept\n",
    "chemical_output_path = './outputs/rescnn/results/bc8biored-chemical-aio/lightweight_cnn_text_biolinkbert/lr_0.001-depth_3-fs_256-drop_0.25/predictions_dev_chemical.json'\n",
    "with open(chemical_output_path, 'r') as f:\n",
    "    chemical_rescnn = json.load(f)\n",
    "chemical_rescnn_dict = {d['mention']: d['pred_id'] for d in chemical_rescnn}\n",
    "print(len(chemical_rescnn), len(chemical_rescnn_dict), chemical_rescnn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Match ResCNN-based Disease/Chemical output with NER predictions\n",
    "\n",
    "for doc_key, values in preds.items():\n",
    "#     print(doc_key)\n",
    "    for pred in values:\n",
    "        # ResCNN only cares about Disease and Chemical\n",
    "        if pred['type'] not in ['DiseaseOrPhenotypicFeature', 'ChemicalEntity']:\n",
    "            continue\n",
    "            \n",
    "        if pred['type'] == 'DiseaseOrPhenotypicFeature':\n",
    "#             print(pred['text'], normalized_query)\n",
    "            normalized_query = mention_map_disease[pred['text']]\n",
    "            if normalized_query not in disease_rescnn_dict:  # composite would not exist\n",
    "                continue     \n",
    "            cui_pred = disease_rescnn_dict[normalized_query]\n",
    "            \n",
    "        elif pred['type'] == 'ChemicalEntity':\n",
    "            normalized_query = mention_map_chemical[pred['text']]\n",
    "            if normalized_query not in chemical_rescnn_dict:  # composite would not exist\n",
    "                continue     \n",
    "            cui_pred = chemical_rescnn_dict[normalized_query]\n",
    "            \n",
    "        cui_pred_list = cui_pred.split('|')\n",
    "        pred['cui'] = cui_pred_list[0] # Only choose the primary ID\n",
    "        \n",
    "#         if len(cui_pred_list) <= 1:    \n",
    "#             pred['cui'] = cui_pred.split('|')[0]          \n",
    "#         else:\n",
    "#             pred['cui'] = '|'.join(cui_pred.split('|')[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mentions: 3496\n",
      "ResCNN matched: 1653\n",
      "PubT3 matched: 1609\n",
      "PubT3-lookup matched: 79\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Match PubT3 output with NER output\n",
    "\n",
    "num_total = 0\n",
    "num_rescnn = 0\n",
    "num_pubt3_em = 0\n",
    "num_pubt3_lookup = 0\n",
    "\n",
    "for doc_key, value_list in pubtator_files.items():  # PubT outputs (NER+NEL)\n",
    "    \n",
    "    target_pred = preds[doc_key]  # Our NER output\n",
    "#     print(doc_key)\n",
    "    \n",
    "    for p in target_pred:\n",
    "        \n",
    "        num_total += 1\n",
    "        \n",
    "        # already matched with ResCNN preds (Disease and Chem, non-composites)\n",
    "        if 'cui' in p:\n",
    "            num_rescnn += 1\n",
    "            continue\n",
    "        \n",
    "        flag = False\n",
    "        for v in value_list:\n",
    "            # exact or partial match for span+type based on PubT3 outputs\n",
    "            if (v['start'] == p['start'] and v['end'] == p['end'] and pubt2biored[v['type']] == p['type']) \\\n",
    "            or (v['end'] >= p['start'] and v['start'] <= p['end'] and pubt2biored[v['type']] == p['type']):\n",
    "                p['cui'] = v['cui']  # follow PubT3 style\n",
    "                flag = True\n",
    "                num_pubt3_em += 1\n",
    "                break\n",
    "                \n",
    "        if not flag:\n",
    "            # Lookup for mention throughout all PubT3 outputs\n",
    "            flag_lookup = False\n",
    "            query = (p['text'].lower(), p['type'])\n",
    "            for (key_mention, key_type), key_cui in pubt_cui_lookup_listed.items():\n",
    "                if query == (key_mention, key_type) or \\\n",
    "                (p['text'].lower() in key_mention or key_mention in p['text'].lower()) and p['type'] == key_type:\n",
    "                    num_pubt3_lookup += 1\n",
    "                    p['cui'] = key_cui\n",
    "                    flag_lookup = True\n",
    "                    break\n",
    "                    \n",
    "            if not flag_lookup:\n",
    "                p['cui'] = \"\"\n",
    "\n",
    "print('Total mentions:', num_total)\n",
    "print('ResCNN matched:', num_rescnn)\n",
    "print('PubT3 matched:', num_pubt3_em)\n",
    "print('PubT3-lookup matched:', num_pubt3_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align NEL outputs for processed json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 450.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "PUBTATOR_ENTITY_TYPES = [\n",
    "    \"GeneOrGeneProduct\", \"DiseaseOrPhenotypicFeature\", \"ChemicalEntity\", \n",
    "    \"OrganismTaxon\", \"SequenceVariant\", \"CellLine\"\n",
    "]\n",
    "\n",
    "for data in tqdm(dev):\n",
    "\n",
    "    data['pred_ner'] = {}\n",
    "    \n",
    "    pred_annotation = preds[data['doc_key']]\n",
    "    for ann in pred_annotation:\n",
    "#         if ann['type'] not in PUBTATOR_ENTITY_TYPES:\n",
    "#             continue\n",
    "        if not ann['cui'] or ann['cui'] == '-':  # cui-less\n",
    "            continue\n",
    "        \n",
    "        start, end = ann['start'], ann['end']\n",
    "        for (char_start, char_end), token_idx in data['char2token'].items():\n",
    "            if char_start <= start < char_end:\n",
    "                token_start = token_idx\n",
    "            if char_start <= end-1 < char_end:\n",
    "                token_end = token_idx\n",
    "                \n",
    "        for idx, (sent_token_start, sent_token_end) in enumerate(data['sentence_spans']):\n",
    "            if sent_token_start <= token_start < sent_token_end:\n",
    "                sent_idx = idx\n",
    "                break\n",
    "                \n",
    "        if ann['type'] != \"SequenceVariant\":\n",
    "            cuis = re.split(r'[,;|]', ann['cui'])\n",
    "        else:\n",
    "            cuis = [ann['cui']]\n",
    "        \n",
    "        ann['database_id'] = ann['cui']\n",
    "        \n",
    "        for cui in cuis:       \n",
    "            if cui not in data['pred_ner']:\n",
    "                value = {\n",
    "                    'entity_id': cui,\n",
    "                    'mention_spans': [[token_start, token_end]],\n",
    "                    'entity_type': ann['type'],\n",
    "                    'sentence_mentions': [sent_idx],\n",
    "                }\n",
    "                data['pred_ner'][cui] = value\n",
    "            else:\n",
    "                target_ann = data['pred_ner'][cui]\n",
    "                target_ann['mention_spans'].append([token_start, token_end])\n",
    "                if sent_idx not in target_ann['sentence_mentions']:\n",
    "                    target_ann['sentence_mentions'].append(sent_idx)\n",
    "          \n",
    "    data['pred_ner'] = list(data['pred_ner'].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 14991.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GeneOrGeneProduct': {'n_gold': 436, 'n_pred': 396, 'n_correct': 344}, 'DiseaseOrPhenotypicFeature': {'n_gold': 344, 'n_pred': 357, 'n_correct': 311}, 'OrganismTaxon': {'n_gold': 113, 'n_pred': 113, 'n_correct': 112}, 'ChemicalEntity': {'n_gold': 220, 'n_pred': 226, 'n_correct': 188}, 'SequenceVariant': {'n_gold': 139, 'n_pred': 118, 'n_correct': 78}, 'CellLine': {'n_gold': 22, 'n_pred': 20, 'n_correct': 17}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NEL Evaluation with F-measure\n",
    "# (pmid, entity_type, id) in the ID evaluation. \n",
    "# If a mention has multiple IDs, and different IDs will be expanded into different instances.\n",
    "\n",
    "labels = list(biored2pubt.keys())\n",
    "\n",
    "nel_scores = {\n",
    "    label: {\n",
    "        \"n_gold\":0, \"n_pred\":0, \"n_correct\":0\n",
    "    } for label in labels\n",
    "}\n",
    "\n",
    "multiple_id_counter = Counter()\n",
    "\n",
    "for data in tqdm(dev):\n",
    "    golds = []  # Doc-level\n",
    "    for gold in data[\"ner\"]:\n",
    "        # Consider multiple ID cases\n",
    "        if gold['entity_type'] != \"SequenceVariant\":\n",
    "            gold_cuis = re.split(r'[,|;]', gold[\"entity_id\"])\n",
    "            gold_cuis = [str(cui.strip()) for cui in gold_cuis]\n",
    "        else:\n",
    "            gold_cuis = [gold[\"entity_id\"]] \n",
    "            \n",
    "        multiple_id_counter[str(len(gold_cuis))] += 1\n",
    "        \n",
    "        for cui in gold_cuis:\n",
    "            if (gold['entity_type'], cui) not in golds:\n",
    "                golds.append((gold['entity_type'], cui))\n",
    "                nel_scores[gold['entity_type']]['n_gold'] += 1\n",
    "        \n",
    "    for pred in data[\"pred_ner\"]:\n",
    "        pred_tuple = (pred['entity_type'], pred['entity_id'])\n",
    "        nel_scores[pred['entity_type']]['n_pred'] += 1\n",
    "        \n",
    "#         if pred['entity_type'] == \"SequenceVariant\":\n",
    "#             print(pred)\n",
    "            \n",
    "        if pred_tuple in golds:\n",
    "            nel_scores[pred['entity_type']]['n_correct'] += 1\n",
    "            \n",
    "#             if pred['entity_type'] == \"SequenceVariant\":\n",
    "#                 print(golds, '\\n')\n",
    "            \n",
    "print(nel_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GeneOrGeneProduct': {'n_gold': 436, 'n_pred': 396, 'n_correct': 344, 'precision': 0.8686868686868687, 'recall': 0.7889908256880734, 'f1': 0.8269230769230771}, 'DiseaseOrPhenotypicFeature': {'n_gold': 344, 'n_pred': 357, 'n_correct': 311, 'precision': 0.8711484593837535, 'recall': 0.9040697674418605, 'f1': 0.8873038516405135}, 'OrganismTaxon': {'n_gold': 113, 'n_pred': 113, 'n_correct': 112, 'precision': 0.9911504424778761, 'recall': 0.9911504424778761, 'f1': 0.9911504424778761}, 'ChemicalEntity': {'n_gold': 220, 'n_pred': 226, 'n_correct': 188, 'precision': 0.831858407079646, 'recall': 0.8545454545454545, 'f1': 0.8430493273542601}, 'SequenceVariant': {'n_gold': 139, 'n_pred': 118, 'n_correct': 78, 'precision': 0.6610169491525424, 'recall': 0.5611510791366906, 'f1': 0.6070038910505836}, 'CellLine': {'n_gold': 22, 'n_pred': 20, 'n_correct': 17, 'precision': 0.85, 'recall': 0.7727272727272727, 'f1': 0.8095238095238095}}\n"
     ]
    }
   ],
   "source": [
    "def save_div(x, y):\n",
    "    if y == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return x/y\n",
    "            \n",
    "for k, v in nel_scores.items():\n",
    "    v[\"precision\"] = save_div(v[\"n_correct\"], v[\"n_pred\"])\n",
    "    v[\"recall\"] = save_div(v[\"n_correct\"], v[\"n_gold\"])\n",
    "    v[\"f1\"] = save_div(2*v[\"precision\"]*v[\"recall\"], (v[\"precision\"]+v[\"recall\"]))\n",
    "\n",
    "print(nel_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneOrGeneProduct(436) >>> prec: 86.87 | rec: 78.90 | f1: 82.69\n",
      "\n",
      "DiseaseOrPhenotypicFeature(344) >>> prec: 87.11 | rec: 90.41 | f1: 88.73\n",
      "\n",
      "OrganismTaxon(113) >>> prec: 99.12 | rec: 99.12 | f1: 99.12\n",
      "\n",
      "ChemicalEntity(220) >>> prec: 83.19 | rec: 85.45 | f1: 84.30\n",
      "\n",
      "SequenceVariant(139) >>> prec: 66.10 | rec: 56.12 | f1: 60.70\n",
      "\n",
      "CellLine(22) >>> prec: 85.00 | rec: 77.27 | f1: 80.95\n",
      "\n",
      "=========================================================================================\n",
      "Total Precision: 85.37 (1050 out of 1230)\n",
      "Total Recall: 82.42 (1050 out of 1274)\n",
      "Total F1: 83.87\n"
     ]
    }
   ],
   "source": [
    "gold = 0\n",
    "correct = 0\n",
    "pred = 0\n",
    "\n",
    "for entity_type, score in nel_scores.items():\n",
    "    gold += score['n_gold']\n",
    "    pred += score['n_pred']\n",
    "    correct += score['n_correct']\n",
    "    \n",
    "    print(f\"{entity_type}({score['n_gold']}) >>> prec: {score['precision']*100:.2f} | rec: {score['recall']*100:.2f} | f1: {score['f1']*100:.2f}\")\n",
    "    print()\n",
    "    \n",
    "print(\"=\"*89)\n",
    "\n",
    "precision = save_div(correct, pred)\n",
    "recall = save_div(correct, gold)\n",
    "f1 = save_div(2*precision*recall, precision+recall)\n",
    "\n",
    "print(f\"Total Precision: {precision*100:.2f} ({correct} out of {pred})\")\n",
    "print(f\"Total Recall: {recall*100:.2f} ({correct} out of {gold})\")\n",
    "print(f\"Total F1: {f1*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the output file for RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assign empty string to entities failed to be normalized\n",
    "pred_output = []\n",
    "preds_ = copy.deepcopy(preds)\n",
    "for k, v in preds_.items():\n",
    "    for e in v:\n",
    "        if 'mention' in e:\n",
    "            del e['mention']\n",
    "        if 'cui' in e:\n",
    "            del e['cui']\n",
    "        if 'database_id' not in e:\n",
    "            e['database_id'] = \"\"\n",
    "        pred_output.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = \"./outputs/final/nel_predictions.csv\"\n",
    "pd.DataFrame(pred_output).to_csv(\"./data/nel_output/dev_100_predictions_nel_ResCNN_DC_PubT3_BioLinkBERT_goldner_032124.csv\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['doc_key', 'sentence_texts', 'sentence_spans', 'tokens', 'token_indices', 'ner', 'relations', 'no_relations', 'pred_ner'])\n"
     ]
    }
   ],
   "source": [
    "# Drop unused keys for RE input\n",
    "unused_keys = ['cased_tokens', 'char2token', 'token2char']\n",
    "for data in dev:\n",
    "    for k in unused_keys:\n",
    "        del data[k]\n",
    "print(dev[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = \"./outputs/final/nel_predictions.json\"\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    for data in dev:\n",
    "        f.write(json.dumps(data))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pernut]",
   "language": "python",
   "name": "conda-env-pernut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
