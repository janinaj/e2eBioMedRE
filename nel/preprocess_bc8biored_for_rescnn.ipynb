{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "import copy\n",
    "\n",
    "from bioc import pubtator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ontologies for Disease\n",
    "- Mainly constructed from MEDIC vocab\n",
    "- Add train samples for Dev Ontology, and Add train+dev for Test ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocess():\n",
    "    \"\"\"\n",
    "    Text Preprocess module\n",
    "    Support lowercase, removing punctuation, typo correction\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            lowercase=True, \n",
    "            remove_punctuation=True,\n",
    "            ignore_punctuations=\"\",\n",
    "            typo_path=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ==========\n",
    "        typo_path : str\n",
    "            path of known typo dictionary\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.typo_path = typo_path\n",
    "        self.rmv_puncts = remove_punctuation\n",
    "        self.punctuation = punctuation\n",
    "        for ig_punc in ignore_punctuations:\n",
    "            self.punctuation = self.punctuation.replace(ig_punc,\"\")\n",
    "        self.rmv_puncts_regex = re.compile(r'[\\s{}]+'.format(re.escape(self.punctuation)))\n",
    "        \n",
    "        if typo_path:\n",
    "            self.typo2correction = self.load_typo2correction(typo_path)\n",
    "        else:\n",
    "            self.typo2correction = {}\n",
    "\n",
    "    def load_typo2correction(self, typo_path):\n",
    "        typo2correction = {}\n",
    "        with open(typo_path, mode='r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                s = line.strip()\n",
    "                tokens = s.split(\"||\")\n",
    "                value = \"\" if len(tokens) == 1 else tokens[1]\n",
    "                typo2correction[tokens[0]] = value    \n",
    "        return typo2correction \n",
    "\n",
    "    def remove_punctuation(self,phrase):\n",
    "        phrase = self.rmv_puncts_regex.split(phrase)\n",
    "        phrase = ' '.join(phrase).strip()\n",
    "        return phrase\n",
    "\n",
    "    def correct_spelling(self, phrase):\n",
    "        phrase_tokens = phrase.split()\n",
    "        phrase = \"\"\n",
    "\n",
    "        for phrase_token in phrase_tokens:\n",
    "            if phrase_token in self.typo2correction.keys():\n",
    "                phrase_token = self.typo2correction[phrase_token]\n",
    "            phrase += phrase_token + \" \"\n",
    "       \n",
    "        phrase = phrase.strip()\n",
    "        return phrase\n",
    "\n",
    "    def run(self, text):\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.typo_path:\n",
    "            text = self.correct_spelling(text)\n",
    "        if self.rmv_puncts:\n",
    "            text = self.remove_punctuation(text)\n",
    "        text = text.strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13300, 9)\n"
     ]
    }
   ],
   "source": [
    "# load MEDIC: the ontology for Disease\n",
    "with open(\"./data/ResCNN/CTD_diseases.tsv\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines = [lines[27]] + lines[29:]\n",
    "lines = [line.split('\\t') for line in lines]\n",
    "df = pd.DataFrame(lines[1:], columns = lines[0])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ontologies_disease(df, text_preprocessor, query_dataset=None, training=True):\n",
    "    \n",
    "    result = {}\n",
    "    mentions = set()\n",
    "\n",
    "    for entdatry in df.to_dict('records'):\n",
    "        alt_ids = []\n",
    "        synonym_list = []\n",
    "\n",
    "        if entry['DiseaseID'].startswith('MESH'):\n",
    "            pid = entry['DiseaseID'].replace('MESH:', '')\n",
    "        elif entry['DiseaseID'].startswith('OMIM'):\n",
    "            pid = entry['DiseaseID'].replace('OMIM:', '')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if entry['AltDiseaseIDs']:\n",
    "            alt_ids.extend(entry['AltDiseaseIDs'].split('|'))\n",
    "            alt_ids = [\n",
    "                alt.replace('MESH:', '').replace('OMIM:', '') for alt in alt_ids if alt.startswith('MESH') or alt.startswith('OMIM')\n",
    "            ]\n",
    "               \n",
    "        # Primary\n",
    "        name_normalized = text_preprocessor.run(entry['# DiseaseName'])\n",
    "        \n",
    "        # Synonym/secondary\n",
    "        if entry['Synonyms']:\n",
    "            synonym_list = entry['Synonyms'].split('|')\n",
    "            synonym_list_normalized = [\n",
    "                text_preprocessor.run(syn) for syn in synonym_list\n",
    "            ]\n",
    "        \n",
    "        if training:  # Separate main id and alt ids\n",
    "            key_ids = [pid] + alt_ids\n",
    "            for key in key_ids:\n",
    "                result[key] = []\n",
    "                result[key].append(\n",
    "                    ['primary', name_normalized]\n",
    "                )\n",
    "                mentions.add(name_normalized)\n",
    "                if entry['Synonyms']:\n",
    "                    for syn_normalized in synonym_list_normalized:\n",
    "                        result[key].append(\n",
    "                            ['synonym/secondary', syn_normalized]\n",
    "                        )\n",
    "                        mentions.add(syn_normalized)\n",
    "                        \n",
    "        else:  # Linearized IDs(chained format)\n",
    "            full_id = '|'.join([pid] + alt_ids)\n",
    "            result[full_id] = []\n",
    "            result[full_id].append(\n",
    "                ['primary', name_normalized]\n",
    "            )\n",
    "            mentions.add(name_normalized)\n",
    "            if entry['Synonyms']:\n",
    "                for syn_normalized in synonym_list_normalized:\n",
    "                    result[full_id].append(\n",
    "                        ['synonym/secondary', syn_normalized]\n",
    "                    )\n",
    "                    mentions.add(syn_normalized)\n",
    "                    \n",
    "    # For DEV ontologies: incorporate id-mention from Train set\n",
    "    # For TEST ontologies: incorporate id-mention from Train+Dev set\n",
    "    if query_dataset and not training:\n",
    "        concept_files = []\n",
    "        for data_dir in query_dataset:\n",
    "            concept_files.extend(glob.glob(os.path.join(data_dir, \"*.concept\")))\n",
    "            \n",
    "        concept_files = {\n",
    "            'bc8biored': [fn for fn in concept_files if 'bc8biored' in fn],\n",
    "            'additional': [fn for fn in concept_files if not 'bc8biored' in fn]\n",
    "        }\n",
    "        \n",
    "        print(len(concept_files['bc8biored']), len(concept_files['additional']))\n",
    "        \n",
    "        for dataset_name in ['bc8biored', 'additional']:  # prioritize bc8 first\n",
    "            for concept_file in concept_files[dataset_name]:\n",
    "                with open(concept_file, 'r') as f:\n",
    "                    doc_queries = f.readlines()\n",
    "\n",
    "                for query in doc_queries:\n",
    "                    _, _, _, name, cui = query.strip().split(\"||\")\n",
    "                    cui = cui.replace('+', '|')\n",
    "                    name = text_preprocessor.run(name)\n",
    "\n",
    "                    if name not in mentions:\n",
    "                        if cui in result:\n",
    "                            result[cui].append(\n",
    "                                ['synonym/secondary', name]\n",
    "                            )\n",
    "                            mentions.add(name)\n",
    "                        else:\n",
    "                            result[cui] = []\n",
    "                            result[cui].append(\n",
    "                                ['primary', name]\n",
    "                            )\n",
    "                            mentions.add(name)\n",
    "                                 \n",
    "    print('Number of Keys >>', len(result))\n",
    "    print('Number of Unique Mentions >>', len(mentions))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Keys >> 17335\n",
      "Number of Unique Mentions >> 85388\n"
     ]
    }
   ],
   "source": [
    "text_preprocessor = TextPreprocess()\n",
    "ontology_train = build_ontologies_disease(\n",
    "    df, text_preprocessor, training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 2292\n",
      "Number of Keys >> 13780\n",
      "Number of Unique Mentions >> 88471\n"
     ]
    }
   ],
   "source": [
    "# Incorporate id-word pairs from ncbi-disease and cdr for DEV set\n",
    "query_dataset = [\n",
    "    './data/biosyn-processed-bc8biored-disease/train/',\n",
    "    './data/biosyn-processed-ncbi-disease/processed_train/',\n",
    "    './data/biosyn-processed-ncbi-disease/processed_dev/',\n",
    "    './data/biosyn-processed-ncbi-disease/processed_test/',\n",
    "    './data/biosyn-processed-bc5cdr-disease/processed_train/',\n",
    "    './data/biosyn-processed-bc5cdr-disease/processed_dev/',\n",
    "    './data/biosyn-processed-bc5cdr-disease/processed_test/',   \n",
    "]\n",
    "ontology_dev = build_ontologies_disease(\n",
    "    df, text_preprocessor, query_dataset=query_dataset, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 2292\n",
      "Number of Keys >> 13786\n",
      "Number of Unique Mentions >> 88577\n"
     ]
    }
   ],
   "source": [
    "# Incorporate id-word pairs from DEV set for TEST set\n",
    "query_dataset.append('./data/biosyn-processed-bc8biored-disease/dev/')\n",
    "ontology_test = build_ontologies_disease(\n",
    "    df, text_preprocessor, query_dataset=query_dataset, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ontologies/bc8biored-disease-aio_train.json', 'w') as f:\n",
    "    f.write(json.dumps(ontology_train))\n",
    "with open('./ontologies/bc8biored-disease-aio_dev.json', 'w') as f:\n",
    "    f.write(json.dumps(ontology_dev))\n",
    "with open('./ontologies/bc8biored-disease-aio.json', 'w') as f:\n",
    "    f.write(json.dumps(ontology_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ontologies for Chemical\n",
    "- Mainly constructed from MeSH vocab\n",
    "- Add train samples for Dev Ontology, and Add train+dev for Test ontology (Follow the same process with Disease-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CTD: the ontology for Chemical\n",
    "with open(\"./data/CTD_chemicals.tsv\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines = [lines[27]] + lines[29:]\n",
    "lines = [line.strip().split('\\t')[:8] for line in lines]\n",
    "df = pd.DataFrame(lines[1:], columns = lines[0])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ontologies_chemical(df, text_preprocessor, query_dataset=None):\n",
    "    \n",
    "    result = {}\n",
    "    mentions = set()\n",
    "\n",
    "    for entry in df.to_dict('records'):\n",
    "\n",
    "        synonym_list = []\n",
    "\n",
    "        if entry['ChemicalID'].startswith('MESH'):\n",
    "            pid = entry['ChemicalID'].replace('MESH:', '')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Primary\n",
    "        name_normalized = text_preprocessor.run(entry['# ChemicalName'])\n",
    "        \n",
    "        # Synonym/secondary\n",
    "        if entry['Synonyms'] is not None:\n",
    "            synonym_list = entry['Synonyms'].split('|')\n",
    "            synonym_list_normalized = [\n",
    "                text_preprocessor.run(syn) for syn in synonym_list\n",
    "            ]\n",
    "        \n",
    "        result[pid] = []\n",
    "        result[pid].append(\n",
    "            ['primary', name_normalized]\n",
    "        )\n",
    "        mentions.add(name_normalized)\n",
    "        \n",
    "        if entry['Synonyms'] is not None:\n",
    "            for syn_normalized in synonym_list_normalized:\n",
    "                result[pid].append(\n",
    "                    ['synonym/secondary', syn_normalized]\n",
    "                )\n",
    "                mentions.add(syn_normalized)\n",
    "                    \n",
    "    # For DEV ontologies: incorporate id-mention from Train set\n",
    "    # For TEST ontologies: incorporate id-mention from Train+Dev set\n",
    "    if query_dataset:\n",
    "        concept_files = []\n",
    "        for data_dir in query_dataset:\n",
    "            concept_files.extend(glob.glob(os.path.join(data_dir, \"*.concept\")))\n",
    "            \n",
    "        concept_files = {\n",
    "            'bc8biored': [fn for fn in concept_files if 'bc8biored' in fn],\n",
    "            'additional': [fn for fn in concept_files if not 'bc8biored' in fn]\n",
    "        }\n",
    "        \n",
    "        print(len(concept_files['bc8biored']), len(concept_files['additional']))\n",
    "        \n",
    "        for dataset_name in ['bc8biored', 'additional']:  # prioritize bc8 first\n",
    "            for concept_file in concept_files[dataset_name]:\n",
    "                with open(concept_file, 'r') as f:\n",
    "                    doc_queries = f.readlines()\n",
    "\n",
    "                for query in doc_queries:\n",
    "                    _, _, _, name, cui = query.strip().split(\"||\")\n",
    "                    cui = cui.replace('+', '|')\n",
    "                    name = text_preprocessor.run(name)\n",
    "\n",
    "                    if name not in mentions:\n",
    "                        if cui in result:\n",
    "                            result[cui].append(\n",
    "                                ['synonym/secondary', name]\n",
    "                            )\n",
    "                            mentions.add(name)\n",
    "                        else:\n",
    "                            result[cui] = []\n",
    "                            result[cui].append(\n",
    "                                ['primary', name]\n",
    "                            )\n",
    "                            mentions.add(name)\n",
    "                                 \n",
    "    print('Number of Keys >>', len(result))\n",
    "    print('Number of Unique Mentions >>', len(mentions))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocess()\n",
    "ontology_train = build_ontologies(\n",
    "    df, text_preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = [\n",
    "    './data/biosyn-processed-bc8biored-chemical/train/',\n",
    "    './data/biosyn-processed-bc5cdr-chemical/processed_train/',\n",
    "    './data/biosyn-processed-bc5cdr-chemical/processed_dev/',\n",
    "    './data/biosyn-processed-bc5cdr-chemical/processed_test/',   \n",
    "]\n",
    "ontology_dev = build_ontologies(\n",
    "    df, text_preprocessor, query_dataset=query_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset.append('./data/biosyn-processed-bc8biored-chemical/dev/')\n",
    "ontology_test = build_ontologies(\n",
    "    df, text_preprocessor, query_dataset=query_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ontologies/bc8biored-chemical-aio_train.json', 'w') as f:\n",
    "    f.write(json.dumps(ontology_train))\n",
    "with open('./ontologies/bc8biored-chemical-aio_dev.json', 'w') as f:\n",
    "    f.write(json.dumps(ontology_dev))\n",
    "with open('./ontologies/bc8biored-chemical-aio.json', 'w') as f:\n",
    "    f.write(json.dumps(ontology_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Query Data for Disease concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BioRED processed datasets from BioSyn package\n",
    "# then, create pairs of mention-ID\n",
    "\n",
    "def load_data(data_dir, filter_composite=True, filter_duplicate=True, filter_cuiless=True):\n",
    "    \"\"\"       \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        a path of data\n",
    "    filter_composite : bool\n",
    "        filter composite mentions\n",
    "    filter_duplicate : bool\n",
    "        filter duplicate queries  \n",
    "    filter_cuiless : bool\n",
    "        remove samples with cuiless \n",
    "    Returns\n",
    "    -------\n",
    "    data : np.array \n",
    "        mention, cui pairs\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    concept_files = glob.glob(os.path.join(data_dir, \"*.concept\"))\n",
    "    for concept_file in tqdm(concept_files):\n",
    "        with open(concept_file, \"r\", encoding='utf-8') as f:\n",
    "            concepts = f.readlines()\n",
    "\n",
    "        for concept in concepts:\n",
    "            concept = concept.split(\"||\")\n",
    "            mention = concept[3].strip()\n",
    "            cui = concept[4].strip()\n",
    "            is_composite = (cui.replace(\"+\",\"|\").count(\"|\") > 0)\n",
    "\n",
    "            # filter composite cui\n",
    "            if filter_composite and is_composite:\n",
    "                continue\n",
    "            # filter cuiless\n",
    "            if filter_cuiless and (cui in ['-1', '-'] or not cui):\n",
    "                continue\n",
    "\n",
    "            data.append((mention,cui))\n",
    "\n",
    "    if filter_duplicate:\n",
    "        data = list(dict.fromkeys(data))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 490/490 [00:00<00:00, 7872.13it/s]\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 12814.08it/s]\n",
      "100%|███████████████████████████████████████| 9668/9668 [00:01<00:00, 6971.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Here, input data is already processed with Biosyn\n",
    "\n",
    "data_dir = './data/biosyn-processed-bc8biored-disease/'\n",
    "pairs_disease = {\n",
    "    'train': load_data(os.path.join(data_dir, 'train')),\n",
    "    'dev': load_data(os.path.join(data_dir, 'dev')),\n",
    "    'test': load_data(os.path.join(data_dir, 'test'), filter_cuiless=False),\n",
    "#     'test': load_data(os.path.join(data_dir, 'dev_pred'), filter_cuiless=False)  # For dev preds (= BioRED Test set)   \n",
    "#     'test': load_data(os.path.join(data_dir, 'test_pred'), filter_cuiless=False)  # For test preds (No gold standard)\n",
    "}\n",
    "pairs_disease['traindev'] = list(dict.fromkeys(pairs_disease['train']+pairs_disease['dev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1313\n",
      "dev 379\n",
      "test 14191\n",
      "traindev 1559\n"
     ]
    }
   ],
   "source": [
    "for k, v in pairs_disease.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 592/592 [00:00<00:00, 3053.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCBI-D-train 1404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:00<00:00, 3511.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCBI-D-dev 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:00<00:00, 3444.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCBI-D-test 340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Disease mentions from CDR and NCBI dataset for training set\n",
    "additional_pairs = []\n",
    "ncbi_d_dir = './data/ResCNN/biosyn-processed-ncbi-disease/'\n",
    "for folder_name in ['train', 'dev', 'test']:\n",
    "    result = load_data(os.path.join(ncbi_d_dir, f'processed_{folder_name}'))\n",
    "    print(f\"NCBI-D-{folder_name}\", len(result))\n",
    "    additional_pairs.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 10121.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDR-D-train 1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 10166.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDR-D-dev 1138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 11689.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDR-D-test 1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Disease mentions from CDR and NCBI dataset for training set\n",
    "cdr_d_dir = './data/ResCNN/biosyn-processed-bc5cdr-disease/'\n",
    "for folder_name in ['train', 'dev', 'test']:\n",
    "    result = load_data(os.path.join(cdr_d_dir, f'processed_{folder_name}'))\n",
    "    print(f\"CDR-D-{folder_name}\", len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049\n",
      "1779\n"
     ]
    }
   ],
   "source": [
    "# filter out duplicated pairs from additional dataset\n",
    "print(len(additional_pairs))\n",
    "additional_pairs = list(dict.fromkeys(additional_pairs))\n",
    "print(len(additional_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional: ('carcinomas', 'D009369')\n",
      "BC8BioRED: ('carcinomas', 'D002277')\n",
      "Additional: ('neurological abnormalities', 'D009461')\n",
      "BC8BioRED: ('neurological abnormalities', 'D009422')\n",
      "Additional: ('neurologic deterioration', 'D009461')\n",
      "BC8BioRED: ('neurologic deterioration', 'D009422')\n",
      "Additional: ('colon carcinoma', 'D003110')\n",
      "BC8BioRED: ('colon carcinoma', 'D015179')\n",
      "Additional: ('neuronal dysfunction', 'D009461')\n",
      "BC8BioRED: ('neuronal dysfunction', 'D009410')\n",
      "Additional: ('neurodegeneration', 'D019636')\n",
      "BC8BioRED: ('neurodegeneration', 'D009422')\n",
      "Additional: ('pelizaeus merzbacher disease', 'D020371')\n",
      "BC8BioRED: ('pelizaeus merzbacher disease', '312080')\n",
      "Additional: ('wolfram syndrome', '222300')\n",
      "BC8BioRED: ('wolfram syndrome', 'D014929')\n",
      "Additional: ('hemochromatosis', 'D016399')\n",
      "BC8BioRED: ('hemochromatosis', 'D006432')\n",
      "Additional: ('polyposis', 'D011125')\n",
      "BC8BioRED: ('polyposis', 'D044483')\n",
      "Additional: ('infertility', 'D007246')\n",
      "BC8BioRED: ('infertility', 'D007247')\n",
      "Additional: ('colorectal adenomas', 'D000236')\n",
      "BC8BioRED: ('colorectal adenomas', 'D003123')\n",
      "Additional: ('thrombocytopenic', 'D013921')\n",
      "BC8BioRED: ('thrombocytopenic', 'D011696')\n",
      "Additional: ('clp', 'D002971')\n",
      "BC8BioRED: ('clp', 'D002972')\n",
      "Additional: ('spastic paraplegia', '312920')\n",
      "BC8BioRED: ('spastic paraplegia', 'D010264')\n",
      "Additional: ('autosomal recessive ataxia', '610743')\n",
      "BC8BioRED: ('autosomal recessive ataxia', 'D013132')\n",
      "Additional: ('iodide transport defect', '274400')\n",
      "BC8BioRED: ('iodide transport defect', 'C564766')\n",
      "Additional: ('neurogenetic disorder', 'D020271')\n",
      "BC8BioRED: ('neurogenetic disorder', 'D009422')\n",
      "Additional: ('cerebral atrophy', 'D001284')\n",
      "BC8BioRED: ('cerebral atrophy', 'D001927')\n",
      "Additional: ('dysmorphic features', 'D057215')\n",
      "BC8BioRED: ('dysmorphic features', 'D000013')\n",
      "Additional: ('neurologic deterioration', 'D019636')\n",
      "BC8BioRED: ('neurologic deterioration', 'D009422')\n",
      "Additional: ('renal lesions', 'D007680')\n",
      "BC8BioRED: ('renal lesions', 'D007674')\n",
      "Additional: ('nystagmus', 'D020417')\n",
      "BC8BioRED: ('nystagmus', 'C580539')\n",
      "Additional: ('developmental defects', 'D002658')\n",
      "BC8BioRED: ('developmental defects', 'D003147')\n",
      "Additional: ('neisseria meningitidis', 'D008589')\n",
      "BC8BioRED: ('neisseria meningitidis', 'D006069')\n",
      "Additional: ('isolated thrombocytopenia', 'D013921')\n",
      "BC8BioRED: ('isolated thrombocytopenia', '313900')\n",
      "Additional: ('breast and or ovarian cancer', '604370')\n",
      "BC8BioRED: ('breast and or ovarian cancer', 'D061325')\n",
      "Additional: ('breast ovarian cancer', '604370')\n",
      "BC8BioRED: ('breast ovarian cancer', 'D061325')\n",
      "Additional: ('inherited breast cancer', 'D001943')\n",
      "BC8BioRED: ('inherited breast cancer', 'D061325')\n",
      "Additional: ('hereditary breast cancer', 'D061325')\n",
      "BC8BioRED: ('hereditary breast cancer', 'D001943')\n",
      "Additional: ('schwartz jampel syndrome', 'D010009')\n",
      "BC8BioRED: ('schwartz jampel syndrome', '255800')\n",
      "Additional: ('skeletal dysplasia', 'C535662')\n",
      "BC8BioRED: ('skeletal dysplasia', 'D001848')\n",
      "Additional: ('colorectal adenomas', 'D018256')\n",
      "BC8BioRED: ('colorectal adenomas', 'D003123')\n",
      "Additional: ('nail dystrophy', 'C538333')\n",
      "BC8BioRED: ('nail dystrophy', 'D009260')\n",
      "Additional: ('insulin dependent diabetes mellitus', 'D003922')\n",
      "BC8BioRED: ('insulin dependent diabetes mellitus', '125852')\n",
      "Additional: ('growth retardation', 'D005317')\n",
      "BC8BioRED: ('growth retardation', 'D006130')\n",
      "Additional: ('familial breast and ovarian cancer', '613399')\n",
      "BC8BioRED: ('familial breast and ovarian cancer', 'D061325')\n"
     ]
    }
   ],
   "source": [
    "for mention, cui in additional_pairs:\n",
    "    if cui not in ontology_train:\n",
    "        continue\n",
    "    if mention not in dict(pairs_disease['train']).keys():\n",
    "        pairs_disease['train'].append((mention, cui))\n",
    "    else:\n",
    "        if cui != dict(pairs_disease['train'])[mention]:\n",
    "            print('Additional:', (mention, cui))\n",
    "            print('BC8BioRED:', (mention, dict(pairs_disease['train'])[mention]))\n",
    "            \n",
    "    if mention not in dict(pairs_disease['traindev']).keys():\n",
    "        pairs_disease['traindev'].append((mention, cui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2842\n",
      "dev 379\n",
      "test 14191\n",
      "traindev 3062\n"
     ]
    }
   ],
   "source": [
    "for k, v in pairs_disease.items():\n",
    "    print(k, len(v))\n",
    "    \n",
    "# Given 1779 extra mention-cui pair from cdr/ncbi disease,\n",
    "# Training set extends from 1313 -> 2842\n",
    "# Train+dev from 1559 -> 3062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mentions only in Dev set: 0.5726\n"
     ]
    }
   ],
   "source": [
    "# Check how many mentions are same btwn train and dev\n",
    "train_mentions = set([mention for mention, cui in pairs_disease['train']])\n",
    "dev_mentions = set([mention for mention, cui in pairs_disease['dev']])\n",
    "# print(len(dev_mentions-train_mentions))\n",
    "mentions_dev_only = len(dev_mentions-train_mentions)/len(dev_mentions)\n",
    "print(f'Mentions only in Dev set: {mentions_dev_only:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './bc8biored-disease-aio'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'data.json'), 'w') as f:\n",
    "with open(os.path.join(output_dir, 'data-dev-predicted.json'), 'w') as f:\n",
    "# with open(os.path.join(output_dir, 'data-test-predicted.json'), 'w') as f:\n",
    "    f.write(json.dumps(pairs_disease))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Query Data for Chemical concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/biosyn-processed-bc8biored-chemical/'\n",
    "pairs_chemical = {\n",
    "    'train': load_data(os.path.join(data_dir, 'train')),\n",
    "    'dev': load_data(os.path.join(data_dir, 'dev')),\n",
    "    'test': load_data(os.path.join(data_dir, 'test'), filter_cuiless=False),\n",
    "#     'test': load_data(os.path.join(data_dir, 'dev_pred'), filter_cuiless=False)  # For dev preds (= BioRED Test set)   \n",
    "#     'test': load_data(os.path.join(data_dir, 'test_pred'), filter_cuiless=False)  # For test preds (No gold standard)\n",
    "}\n",
    "pairs_chemical['traindev'] = list(dict.fromkeys(pairs_chemical['train']+pairs_chemical['dev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in pairs_chemical.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Chemical mentions from CDR dataset for training set\n",
    "additional_pairs = []\n",
    "cdr_c_dir = './data/biosyn-processed-bc5cdr-chemical/'\n",
    "for folder_name in ['train', 'dev', 'test']:\n",
    "    result = load_data(os.path.join(cdr_c_dir, f'processed_{folder_name}'))\n",
    "    print(f\"CDR-C-{folder_name}\", len(result))\n",
    "    additional_pairs.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out duplicated pairs from additional dataset\n",
    "print(len(additional_pairs))\n",
    "additional_pairs = list(dict.fromkeys(additional_pairs))\n",
    "print(len(additional_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mention, cui in additional_pairs:\n",
    "    # remove additional mentions that are not in the train ontology\n",
    "    if cui not in ontology_train:\n",
    "        continue\n",
    "    if mention not in dict(pairs_chemical['train']).keys():\n",
    "        pairs_chemical['train'].append((mention, cui))\n",
    "    else:\n",
    "        if cui != dict(pairs_chemical['train'])[mention]:\n",
    "            print('Additional:', (mention, cui))\n",
    "            print('BC8BioRED:', (mention, dict(pairs_chemical['train'])[mention]))\n",
    "            \n",
    "    if mention not in dict(pairs_chemical['traindev']).keys():\n",
    "        pairs_chemical['traindev'].append((mention, cui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in pairs_chemical.items():\n",
    "    print(k, len(v))\n",
    "    \n",
    "# Given 1737 extra mention-cui pair from cdr/ncbi disease,\n",
    "# Training set extends from 723 -> 2008\n",
    "# Train+dev from 825 -> 2047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many mentions are same btwn train and dev\n",
    "train_mentions = set([mention for mention, cui in pairs_chemical['train']])\n",
    "dev_mentions = set([mention for mention, cui in pairs_chemical['dev']])\n",
    "# print(len(dev_mentions-train_mentions))\n",
    "mentions_dev_only = len(dev_mentions-train_mentions)/len(dev_mentions)\n",
    "print(f'Mentions only in Dev set: {mentions_dev_only:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './bc8biored-chemical-aio'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, 'data.json'), 'w') as f:\n",
    "# with open(os.path.join(output_dir, 'data-dev-predicted.json'), 'w') as f:\n",
    "# with open(os.path.join(output_dir, 'data-test-predicted.json'), 'w') as f:\n",
    "    f.write(json.dumps(pairs_chemical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
